{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "written-trauma",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-stress",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "right-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Niels/Documents/EPFL/Master/DeepLearning/project1\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-deadline",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "traditional-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models as models\n",
    "from train import train\n",
    "from evaluation import model_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-science",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline(params):\n",
    "    # No hidden layer units, ignore hidden_units parameter\n",
    "    \n",
    "    model = models.BaselineCNN()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-cutting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 5, lr=0.0001, units=10\n",
      "    Round 0: 0.6015\n",
      "    Round 1: 0.6970\n",
      "    Round 2: 0.7694\n",
      "    Round 3: 0.7049\n",
      "    Round 4: 0.6547\n",
      "  Average: 0.6855\n",
      "Testing batch_size 5, lr=0.001, units=10\n",
      "    Round 0: 1.3536\n",
      "    Round 1: 1.1893\n",
      "    Round 2: 1.0124\n",
      "    Round 3: 0.9906\n",
      "    Round 4: 1.2841\n",
      "  Average: 1.1660\n",
      "Testing batch_size 5, lr=0.01, units=10\n",
      "    Round 0: 0.6954\n",
      "    Round 1: 0.6954\n",
      "    Round 2: 0.6812\n",
      "    Round 3: 0.6864\n",
      "    Round 4: 0.6891\n",
      "  Average: 0.6895\n",
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.5337\n",
      "    Round 1: 0.5477\n",
      "    Round 2: 0.6918\n",
      "    Round 3: 0.4313\n",
      "    Round 4: 0.6364\n",
      "  Average: 0.5682\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 1.1887\n",
      "    Round 1: 1.3569\n",
      "    Round 2: 1.4040\n",
      "    Round 3: 1.2193\n",
      "    Round 4: 0.8856\n",
      "  Average: 1.2109\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6112\n",
      "    Round 1: 0.6704\n",
      "    Round 2: 0.8019\n",
      "    Round 3: 0.8272\n",
      "    Round 4: 0.8827\n",
      "  Average: 0.7587\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.5353\n",
      "    Round 1: 0.5919\n",
      "    Round 2: 0.5865\n",
      "    Round 3: 0.3582\n",
      "    Round 4: 0.6866\n",
      "  Average: 0.5517\n",
      "Testing batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 1.4576\n",
      "    Round 1: 1.0997\n",
      "    Round 2: 1.6800\n",
      "    Round 3: 0.9364\n",
      "    Round 4: 1.1513\n",
      "  Average: 1.2650\n",
      "Testing batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.6682\n",
      "    Round 1: 0.6280\n",
      "    Round 2: 0.8733\n",
      "    Round 3: 0.6473\n",
      "    Round 4: 0.5670\n",
      "  Average: 0.6768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 0.0001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = 5, 10, 25\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10]\n",
    "\n",
    "batch_size, lr, _ = model_tuning(\n",
    "    gen_baseline, train, epochs, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-answer",
   "metadata": {},
   "source": [
    "## Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline_2(params):\n",
    "    # No hidden layer units, ignore parameter\n",
    "    \n",
    "    model = models.BaselineCNN2(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coastal-samoa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 5, lr=0.0001, units=10\n",
      "    Round 0: 0.9576\n",
      "    Round 1: 0.9784\n",
      "    Round 2: 1.1027\n",
      "    Round 3: 1.0911\n",
      "    Round 4: 0.8011\n",
      "  Average: 0.9862\n",
      "Testing batch_size 5, lr=0.0001, units=25\n",
      "    Round 0: 0.7543\n",
      "    Round 1: 0.8645\n",
      "    Round 2: 1.3593\n",
      "    Round 3: 1.4834\n",
      "    Round 4: 0.8232\n",
      "  Average: 1.0569\n",
      "Testing batch_size 5, lr=0.0001, units=50\n",
      "    Round 0: 0.8524\n",
      "    Round 1: 0.7718\n",
      "    Round 2: 1.1971\n",
      "    Round 3: 1.3385\n",
      "    Round 4: 1.1664\n",
      "  Average: 1.0652\n",
      "Testing batch_size 5, lr=0.0001, units=100\n",
      "    Round 0: 0.9310\n",
      "    Round 1: 1.2756\n",
      "    Round 2: 1.4028\n",
      "    Round 3: 1.5334\n",
      "    Round 4: 0.9613\n",
      "  Average: 1.2208\n",
      "Testing batch_size 5, lr=0.001, units=10\n",
      "    Round 0: 1.5645\n",
      "    Round 1: 0.6819\n",
      "    Round 2: 3.0743\n",
      "    Round 3: 2.0154\n",
      "    Round 4: 0.6720\n",
      "  Average: 1.6016\n",
      "Testing batch_size 5, lr=0.001, units=25\n",
      "    Round 0: 1.2003\n",
      "    Round 1: 1.0021\n",
      "    Round 2: 2.9099\n",
      "    Round 3: 3.3612\n",
      "    Round 4: 1.3876\n",
      "  Average: 1.9722\n",
      "Testing batch_size 5, lr=0.001, units=50\n",
      "    Round 0: 1.3418\n",
      "    Round 1: 1.6025\n",
      "    Round 2: 1.3298\n",
      "    Round 3: 1.0586\n",
      "    Round 4: 2.2886\n",
      "  Average: 1.5242\n",
      "Testing batch_size 5, lr=0.001, units=100\n",
      "    Round 0: 1.3123\n",
      "    Round 1: 0.9935\n",
      "    Round 2: 2.7208\n",
      "    Round 3: 2.7521\n",
      "    Round 4: 1.8111\n",
      "  Average: 1.9180\n",
      "Testing batch_size 5, lr=0.01, units=10\n",
      "    Round 0: 0.6836\n",
      "    Round 1: 0.6819\n",
      "    Round 2: 0.7022\n",
      "    Round 3: 0.6863\n",
      "    Round 4: 0.6663\n",
      "  Average: 0.6841\n",
      "Testing batch_size 5, lr=0.01, units=25\n",
      "    Round 0: 0.6821\n",
      "    Round 1: 0.6823\n",
      "    Round 2: 0.6961\n",
      "    Round 3: 0.6859\n",
      "    Round 4: 0.6660\n",
      "  Average: 0.6825\n",
      "Testing batch_size 5, lr=0.01, units=50\n",
      "    Round 0: 0.6821\n",
      "    Round 1: 0.6821\n",
      "    Round 2: 0.6970\n",
      "    Round 3: 0.6869\n",
      "    Round 4: 0.6730\n",
      "  Average: 0.6842\n",
      "Testing batch_size 5, lr=0.01, units=100\n",
      "    Round 0: 0.6822\n",
      "    Round 1: 0.6820\n",
      "    Round 2: 0.7017\n",
      "    Round 3: 0.6882\n",
      "    Round 4: 0.6721\n",
      "  Average: 0.6853\n",
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.6665\n",
      "    Round 1: 0.7028\n",
      "    Round 2: 0.9886\n",
      "    Round 3: 0.9556\n",
      "    Round 4: 0.6957\n",
      "  Average: 0.8018\n",
      "Testing batch_size 10, lr=0.0001, units=25\n",
      "    Round 0: 0.6641\n",
      "    Round 1: 0.7169\n",
      "    Round 2: 1.0035\n",
      "    Round 3: 1.1457\n",
      "    Round 4: 0.8757\n",
      "  Average: 0.8812\n",
      "Testing batch_size 10, lr=0.0001, units=50\n",
      "    Round 0: 0.7127\n",
      "    Round 1: 0.7799\n",
      "    Round 2: 1.0342\n",
      "    Round 3: 1.3940\n",
      "    Round 4: 1.0815\n",
      "  Average: 1.0005\n",
      "Testing batch_size 10, lr=0.0001, units=100\n",
      "    Round 0: 0.9878\n",
      "    Round 1: 0.6557\n",
      "    Round 2: 1.1836\n",
      "    Round 3: 1.0561\n",
      "    Round 4: 0.7677\n",
      "  Average: 0.9302\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 0.8923\n",
      "    Round 1: 1.2865\n",
      "    Round 2: 2.0865\n",
      "    Round 3: 2.4570\n",
      "    Round 4: 1.5169\n",
      "  Average: 1.6478\n",
      "Testing batch_size 10, lr=0.001, units=25\n",
      "    Round 0: 1.5976\n",
      "    Round 1: 1.4169\n",
      "    Round 2: 2.5439\n",
      "    Round 3: 2.7265\n",
      "    Round 4: 1.3535\n",
      "  Average: 1.9277\n",
      "Testing batch_size 10, lr=0.001, units=50\n",
      "    Round 0: 1.1573\n",
      "    Round 1: 1.9306\n",
      "    Round 2: 2.5560\n",
      "    Round 3: 2.1623\n",
      "    Round 4: 1.3964\n",
      "  Average: 1.8405\n",
      "Testing batch_size 10, lr=0.001, units=100\n",
      "    Round 0: 1.4597\n",
      "    Round 1: 1.0309\n",
      "    Round 2: 1.5762\n",
      "    Round 3: 1.8005\n",
      "    Round 4: 1.5679\n",
      "  Average: 1.4870\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6820\n",
      "    Round 1: 0.6820\n",
      "    Round 2: 0.6984\n",
      "    Round 3: 0.6859\n",
      "    Round 4: 0.6720\n",
      "  Average: 0.6841\n",
      "Testing batch_size 10, lr=0.01, units=25\n",
      "    Round 0: 0.5250\n",
      "    Round 1: 0.6819\n",
      "    Round 2: 0.7006\n",
      "    Round 3: 0.6871\n",
      "    Round 4: 0.6720\n",
      "  Average: 0.6533\n",
      "Testing batch_size 10, lr=0.01, units=50\n",
      "    Round 0: 0.6819\n",
      "    Round 1: 0.6819\n",
      "    Round 2: 0.7023\n",
      "    Round 3: 0.6862\n",
      "    Round 4: 0.6744\n",
      "  Average: 0.6853\n",
      "Testing batch_size 10, lr=0.01, units=100\n",
      "    Round 0: 0.6819\n",
      "    Round 1: 0.6819\n",
      "    Round 2: 0.7017\n",
      "    Round 3: 0.6865\n",
      "    Round 4: 0.6719\n",
      "  Average: 0.6848\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.4718\n",
      "    Round 1: 0.4610\n",
      "    Round 2: 0.8283\n",
      "    Round 3: 0.7852\n",
      "    Round 4: 0.6370\n",
      "  Average: 0.6367\n",
      "Testing batch_size 25, lr=0.0001, units=25\n",
      "    Round 0: 0.5480\n",
      "    Round 1: 0.7418\n",
      "    Round 2: 0.7047\n",
      "    Round 3: 0.6912\n",
      "    Round 4: 0.6634\n",
      "  Average: 0.6698\n",
      "Testing batch_size 25, lr=0.0001, units=50\n",
      "    Round 0: 0.4495\n",
      "    Round 1: 0.4849\n",
      "    Round 2: 0.7371\n",
      "    Round 3: 0.8081\n",
      "    Round 4: 0.6124\n",
      "  Average: 0.6184\n",
      "Testing batch_size 25, lr=0.0001, units=100\n",
      "    Round 0: 0.5926\n",
      "    Round 1: 0.5802\n",
      "    Round 2: 0.8310\n",
      "    Round 3: 0.8545\n",
      "    Round 4: 0.7152\n",
      "  Average: 0.7147\n",
      "Testing batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 1.4241\n",
      "    Round 1: 1.2012\n",
      "    Round 2: 1.7530\n",
      "    Round 3: 1.3639\n",
      "    Round 4: 1.4355\n",
      "  Average: 1.4355\n",
      "Testing batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 1.0702\n",
      "    Round 1: 1.3837\n",
      "    Round 2: 1.5750\n",
      "    Round 3: 2.1737\n",
      "    Round 4: 1.6249\n",
      "  Average: 1.5655\n",
      "Testing batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 1.6147\n",
      "    Round 1: 1.2108\n",
      "    Round 2: 1.7559\n",
      "    Round 3: 2.3238\n",
      "    Round 4: 1.3918\n",
      "  Average: 1.6594\n",
      "Testing batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 1.2317\n",
      "    Round 1: 1.5741\n",
      "    Round 2: 1.2766\n",
      "    Round 3: 2.0470\n",
      "    Round 4: 0.9262\n",
      "  Average: 1.4111\n",
      "Testing batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.6749\n",
      "    Round 1: 0.6034\n",
      "    Round 2: 2.2238\n",
      "    Round 3: 1.3151\n",
      "    Round 4: 1.0388\n",
      "  Average: 1.1712\n",
      "Testing batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 1.1044\n",
      "    Round 1: 0.6538\n",
      "    Round 2: 1.3564\n",
      "    Round 3: 0.8904\n",
      "    Round 4: 1.2140\n",
      "  Average: 1.0438\n",
      "Testing batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.7102\n",
      "    Round 1: 0.7096\n",
      "    Round 2: 0.9057\n",
      "    Round 3: 0.7994\n",
      "    Round 4: 0.7299\n",
      "  Average: 0.7710\n",
      "Testing batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.5755\n",
      "    Round 1: 0.7048\n",
      "    Round 2: 0.6980\n",
      "    Round 3: 0.6860\n",
      "    Round 4: 0.6325\n",
      "  Average: 0.6594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 0.0001, 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = 5, 10, 25\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "batch_size, lr, hidden_units = model_tuning(\n",
    "    gen_baseline_2, train, epochs, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-liabilities",
   "metadata": {},
   "source": [
    "## Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "competent-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_weight_sharing(params):\n",
    "    # No hidden layer units, ignore parameter\n",
    "    \n",
    "    model = models.WeightSharingCNN(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "covered-tomorrow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 5, lr=0.0001, units=10\n",
      "    Round 0: 0.7791\n",
      "    Round 1: 0.6100\n",
      "    Round 2: 1.0758\n",
      "    Round 3: 0.8873\n",
      "    Round 4: 0.6716\n",
      "  Average: 0.8048\n",
      "Testing batch_size 5, lr=0.0001, units=25\n",
      "    Round 0: 0.8937\n",
      "    Round 1: 0.7156\n",
      "    Round 2: 1.0696\n",
      "    Round 3: 0.7496\n",
      "    Round 4: 0.8852\n",
      "  Average: 0.8627\n",
      "Testing batch_size 5, lr=0.0001, units=50\n",
      "    Round 0: 0.8810\n",
      "    Round 1: 0.8129\n",
      "    Round 2: 1.1071\n",
      "    Round 3: 0.9494\n",
      "    Round 4: 0.8479\n",
      "  Average: 0.9197\n",
      "Testing batch_size 5, lr=0.0001, units=100\n",
      "    Round 0: 0.9652\n",
      "    Round 1: 1.0826\n",
      "    Round 2: 1.0080\n",
      "    Round 3: 0.8521\n",
      "    Round 4: 1.1544\n",
      "  Average: 1.0124\n",
      "Testing batch_size 5, lr=0.001, units=10\n",
      "    Round 0: 1.2140\n",
      "    Round 1: 1.1916\n",
      "    Round 2: 1.4349\n",
      "    Round 3: 0.6927\n",
      "    Round 4: 1.6243\n",
      "  Average: 1.2315\n",
      "Testing batch_size 5, lr=0.001, units=25\n",
      "    Round 0: 1.4043\n",
      "    Round 1: 1.1313\n",
      "    Round 2: 1.4532\n",
      "    Round 3: 0.9771\n",
      "    Round 4: 1.5055\n",
      "  Average: 1.2943\n",
      "Testing batch_size 5, lr=0.001, units=50\n",
      "    Round 0: 1.0796\n",
      "    Round 1: 0.8269\n",
      "    Round 2: 1.4179\n",
      "    Round 3: 1.3658\n",
      "    Round 4: 0.9984\n",
      "  Average: 1.1377\n",
      "Testing batch_size 5, lr=0.001, units=100\n",
      "    Round 0: 0.9303\n",
      "    Round 1: 1.6770\n",
      "    Round 2: 1.2150\n",
      "    Round 3: 1.3564\n",
      "    Round 4: 1.0787\n",
      "  Average: 1.2515\n",
      "Testing batch_size 5, lr=0.01, units=10\n",
      "    Round 0: 0.6916\n",
      "    Round 1: 0.6867\n",
      "    Round 2: 0.6936\n",
      "    Round 3: 0.6927\n",
      "    Round 4: 0.6901\n",
      "  Average: 0.6909\n",
      "Testing batch_size 5, lr=0.01, units=25\n",
      "    Round 0: 0.6914\n",
      "    Round 1: 0.6868\n",
      "    Round 2: 0.6953\n",
      "    Round 3: 0.6948\n",
      "    Round 4: 0.6904\n",
      "  Average: 0.6917\n",
      "Testing batch_size 5, lr=0.01, units=50\n",
      "    Round 0: 0.6937\n",
      "    Round 1: 0.6871\n",
      "    Round 2: 0.6942\n",
      "    Round 3: 0.6927\n",
      "    Round 4: 0.6908\n",
      "  Average: 0.6917\n",
      "Testing batch_size 5, lr=0.01, units=100\n",
      "    Round 0: 0.6931\n",
      "    Round 1: 0.6920\n",
      "    Round 2: 0.6934\n",
      "    Round 3: 0.6931\n",
      "    Round 4: 0.6905\n",
      "  Average: 0.6924\n",
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.6246\n",
      "    Round 1: 0.5746\n",
      "    Round 2: 0.6758\n",
      "    Round 3: 0.5263\n",
      "    Round 4: 0.5264\n",
      "  Average: 0.5855\n",
      "Testing batch_size 10, lr=0.0001, units=25\n",
      "    Round 0: 0.7541\n",
      "    Round 1: 0.5618\n",
      "    Round 2: 0.5393\n",
      "    Round 3: 0.5328\n",
      "    Round 4: 0.6349\n",
      "  Average: 0.6046\n",
      "Testing batch_size 10, lr=0.0001, units=50\n",
      "    Round 0: 0.8185\n",
      "    Round 1: 0.6437\n",
      "    Round 2: 0.6824\n",
      "    Round 3: 0.6587\n",
      "    Round 4: 0.6950\n",
      "  Average: 0.6997\n",
      "Testing batch_size 10, lr=0.0001, units=100\n",
      "    Round 0: 0.7497\n",
      "    Round 1: 0.7938\n",
      "    Round 2: 0.8748\n",
      "    Round 3: 0.6264\n",
      "    Round 4: 0.7554\n",
      "  Average: 0.7600\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 1.0705\n",
      "    Round 1: 1.4169\n",
      "    Round 2: 1.2488\n",
      "    Round 3: 1.1147\n",
      "    Round 4: 0.9450\n",
      "  Average: 1.1592\n",
      "Testing batch_size 10, lr=0.001, units=25\n",
      "    Round 0: 1.4641\n",
      "    Round 1: 1.4277\n",
      "    Round 2: 1.6698\n",
      "    Round 3: 0.9614\n",
      "    Round 4: 1.2245\n",
      "  Average: 1.3495\n",
      "Testing batch_size 10, lr=0.001, units=50\n",
      "    Round 0: 1.2341\n",
      "    Round 1: 1.0652\n",
      "    Round 2: 1.6091\n",
      "    Round 3: 1.1665\n",
      "    Round 4: 1.4349\n",
      "  Average: 1.3020\n",
      "Testing batch_size 10, lr=0.001, units=100\n",
      "    Round 0: 1.0951\n",
      "    Round 1: 1.3886\n",
      "    Round 2: 1.9023\n",
      "    Round 3: 1.7390\n",
      "    Round 4: 1.2242\n",
      "  Average: 1.4698\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6916\n",
      "    Round 1: 0.6901\n",
      "    Round 2: 0.6947\n",
      "    Round 3: 0.6924\n",
      "    Round 4: 0.6901\n",
      "  Average: 0.6918\n",
      "Testing batch_size 10, lr=0.01, units=25\n",
      "    Round 0: 0.4812\n",
      "    Round 1: 0.6877\n",
      "    Round 2: 0.6933\n",
      "    Round 3: 0.6889\n",
      "    Round 4: 0.6905\n",
      "  Average: 0.6483\n",
      "Testing batch_size 10, lr=0.01, units=50\n",
      "    Round 0: 0.4544\n",
      "    Round 1: 0.7907\n",
      "    Round 2: 0.6940\n",
      "    Round 3: 0.4647\n",
      "    Round 4: 0.5174\n",
      "  Average: 0.5842\n",
      "Testing batch_size 10, lr=0.01, units=100\n",
      "    Round 0: 0.6915\n",
      "    Round 1: 1.0333\n",
      "    Round 2: 0.6169\n",
      "    Round 3: 0.9128\n",
      "    Round 4: 0.6901\n",
      "  Average: 0.7889\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.4099\n",
      "    Round 1: 0.4605\n",
      "    Round 2: 0.5147\n",
      "    Round 3: 0.4838\n",
      "    Round 4: 0.4675\n",
      "  Average: 0.4673\n",
      "Testing batch_size 25, lr=0.0001, units=25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ba637344da12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhidden_layer_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m batch_size, lr, hidden_units = model_tuning(\n\u001b[0m\u001b[1;32m     10\u001b[0m     gen_weight_sharing, train, epochs, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/DeepLearning/project1/evaluation.py\u001b[0m in \u001b[0;36mmodel_tuning\u001b[0;34m(gen_model, train_method, num_epochs, batch_sizes, learning_rates, hidden_layer_units, seed, print_round_results)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                     \u001b[0mfinal_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0maverage_val_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfinal_val_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/DeepLearning/project1/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_data, test_data, num_epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = 5, 10, 25\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "batch_size, lr, hidden_units = model_tuning(\n",
    "    gen_weight_sharing, train, epochs, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-immune",
   "metadata": {},
   "source": [
    "## Weight Sharing and Auxiliary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-small",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
