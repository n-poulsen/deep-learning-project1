{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "written-trauma",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-stress",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "right-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Niels/Documents/EPFL/Master/DeepLearning/project1\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-deadline",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "traditional-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models as models\n",
    "from train import train\n",
    "from evaluation import model_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-science",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline(params):\n",
    "    # No hidden layer units, ignore hidden_units parameter\n",
    "    \n",
    "    model = models.BaselineCNN()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-cutting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 5, lr=0.0001, units=10\n",
      "    Round 0: 1.1092\n",
      "    Round 1: 0.7969\n",
      "    Round 2: 0.7165\n",
      "    Round 3: 0.7688\n",
      "    Round 4: 0.7141\n",
      "    Round 5: 0.6834\n",
      "    Round 6: 1.2128\n",
      "    Round 7: 0.7013\n",
      "    Round 8: 0.7184\n",
      "    Round 9: 0.9250\n",
      "  Average: 0.8346\n",
      "Testing batch_size 5, lr=0.001, units=10\n",
      "    Round 0: 1.8809\n",
      "    Round 1: 1.2092\n",
      "    Round 2: 2.2841\n",
      "    Round 3: 1.2975\n",
      "    Round 4: 1.4690\n",
      "    Round 5: 0.9899\n",
      "    Round 6: 3.1117\n",
      "    Round 7: 1.3108\n",
      "    Round 8: 1.3600\n",
      "    Round 9: 1.5018\n",
      "  Average: 1.6415\n",
      "Testing batch_size 5, lr=0.01, units=10\n",
      "    Round 0: 0.6871\n",
      "    Round 1: 0.6862\n",
      "    Round 2: 0.6938\n",
      "    Round 3: 0.6946\n",
      "    Round 4: 0.7018\n",
      "    Round 5: 0.6874\n",
      "    Round 6: 0.6833\n",
      "    Round 7: 0.6924\n",
      "    Round 8: 0.6842\n",
      "    Round 9: 0.6942\n",
      "  Average: 0.6905\n",
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.6710\n",
      "    Round 1: 0.4592\n",
      "    Round 2: 0.6989\n",
      "    Round 3: 0.5265\n",
      "    Round 4: 0.7384\n",
      "    Round 5: 0.8710\n",
      "    Round 6: 0.6416\n",
      "    Round 7: 0.6832\n",
      "    Round 8: 0.7944\n",
      "    Round 9: 0.7787\n",
      "  Average: 0.6863\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 1.7528\n",
      "    Round 1: 1.2634\n",
      "    Round 2: 1.5326\n",
      "    Round 3: 1.8392\n",
      "    Round 4: 1.5444\n",
      "    Round 5: 1.1966\n",
      "    Round 6: 1.5006\n",
      "    Round 7: 1.4620\n",
      "    Round 8: 1.2984\n",
      "    Round 9: 1.6435\n",
      "  Average: 1.5034\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6863\n",
      "    Round 1: 0.6228\n",
      "    Round 2: 0.6833\n",
      "    Round 3: 0.7196\n",
      "    Round 4: 0.6965\n",
      "    Round 5: 0.6871\n",
      "    Round 6: 0.6947\n",
      "    Round 7: 0.6861\n",
      "    Round 8: 1.1006\n",
      "    Round 9: 0.6927\n",
      "  Average: 0.7270\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.5113\n",
      "    Round 1: 0.6366\n",
      "    Round 2: 0.6295\n",
      "    Round 3: 0.5159\n",
      "    Round 4: 0.5121\n",
      "    Round 5: 0.5831\n",
      "    Round 6: 0.6528\n",
      "    Round 7: 0.6365\n",
      "    Round 8: 0.6296\n",
      "    Round 9: 0.5520\n",
      "  Average: 0.5859\n",
      "Testing batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 1.2979\n",
      "    Round 1: 1.2085\n",
      "    Round 2: 1.2856\n",
      "    Round 3: 1.5024\n",
      "    Round 4: 1.4923\n",
      "    Round 5: 1.3779\n",
      "    Round 6: 1.4220\n",
      "    Round 7: 1.0885\n",
      "    Round 8: 1.1858\n",
      "    Round 9: 1.2755\n",
      "  Average: 1.3137\n",
      "Testing batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.5239\n",
      "    Round 1: 1.3100\n",
      "    Round 2: 0.6686\n",
      "    Round 3: 1.1543\n",
      "    Round 4: 0.6781\n",
      "    Round 5: 1.4344\n",
      "    Round 6: 1.3751\n",
      "    Round 7: 1.8505\n",
      "    Round 8: 2.1428\n",
      "    Round 9: 0.6837\n",
      "  Average: 1.1821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 0.0001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [5, 10, 25]\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10]\n",
    "\n",
    "batch_size, lr, _ = model_tuning(\n",
    "    gen_baseline, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-answer",
   "metadata": {},
   "source": [
    "## Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline_2(params):\n",
    "    # No hidden layer units, ignore parameter\n",
    "    \n",
    "    model = models.BaselineCNN2(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-samoa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.7136\n",
      "    Round 1: 0.8627\n",
      "    Round 2: 0.7815\n",
      "    Round 3: 0.8811\n",
      "    Round 4: 0.7991\n",
      "    Round 5: 0.9208\n",
      "    Round 6: 1.0042\n",
      "    Round 7: 0.7632\n",
      "    Round 8: 0.9754\n",
      "    Round 9: 0.6918\n",
      "  Average: 0.8393\n",
      "Testing batch_size 10, lr=0.0001, units=25\n",
      "    Round 0: 0.9505\n",
      "    Round 1: 1.2050\n",
      "    Round 2: 1.0164\n",
      "    Round 3: 0.6881\n",
      "    Round 4: 1.2187\n",
      "    Round 5: 1.1406\n",
      "    Round 6: 0.9415\n",
      "    Round 7: 0.7762\n",
      "    Round 8: 0.7240\n",
      "    Round 9: 1.0407\n",
      "  Average: 0.9702\n",
      "Testing batch_size 10, lr=0.0001, units=50\n",
      "    Round 0: 1.3807\n",
      "    Round 1: 0.9928\n",
      "    Round 2: 0.9718\n",
      "    Round 3: 0.8047\n",
      "    Round 4: 0.9663\n",
      "    Round 5: 0.6732\n",
      "    Round 6: 0.6444\n",
      "    Round 7: 0.9131\n",
      "    Round 8: 1.0252\n",
      "    Round 9: 0.8202\n",
      "  Average: 0.9192\n",
      "Testing batch_size 10, lr=0.0001, units=100\n",
      "    Round 0: 0.8572\n",
      "    Round 1: 1.1305\n",
      "    Round 2: 1.2486\n",
      "    Round 3: 1.0130\n",
      "    Round 4: 0.9079\n",
      "    Round 5: 0.9733\n",
      "    Round 6: 0.9500\n",
      "    Round 7: 0.8811\n",
      "    Round 8: 0.6967\n",
      "    Round 9: 0.7953\n",
      "  Average: 0.9453\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 1.6573\n",
      "    Round 1: 2.5511\n",
      "    Round 2: 1.9330\n",
      "    Round 3: 1.3435\n",
      "    Round 4: 1.1243\n",
      "    Round 5: 1.7298\n",
      "    Round 6: 0.6900\n",
      "    Round 7: 1.9214\n",
      "    Round 8: 1.4959\n",
      "    Round 9: 0.6784\n",
      "  Average: 1.5125\n",
      "Testing batch_size 10, lr=0.001, units=25\n",
      "    Round 0: 1.5979\n",
      "    Round 1: 1.6433\n",
      "    Round 2: 1.7043\n",
      "    Round 3: 1.8209\n",
      "    Round 4: 1.5714\n",
      "    Round 5: 1.1264\n",
      "    Round 6: 1.3227\n",
      "    Round 7: 1.8581\n",
      "    Round 8: 1.3866\n",
      "    Round 9: 1.8087\n",
      "  Average: 1.5840\n",
      "Testing batch_size 10, lr=0.001, units=50\n",
      "    Round 0: 1.4713\n",
      "    Round 1: 1.7442\n",
      "    Round 2: 1.4576\n",
      "    Round 3: 1.4567\n",
      "    Round 4: 2.4276\n",
      "    Round 5: 1.6219\n",
      "    Round 6: 1.9430\n",
      "    Round 7: 2.3411\n",
      "    Round 8: 1.5257\n",
      "    Round 9: 1.4640\n",
      "  Average: 1.7453\n",
      "Testing batch_size 10, lr=0.001, units=100\n",
      "    Round 0: 1.7027\n",
      "    Round 1: 2.3635\n",
      "    Round 2: 1.5008\n",
      "    Round 3: 1.8197\n",
      "    Round 4: 1.3949\n",
      "    Round 5: 1.6832\n",
      "    Round 6: 1.7729\n",
      "    Round 7: 1.1171\n",
      "    Round 8: 1.3653\n",
      "    Round 9: 2.4515\n",
      "  Average: 1.7172\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6872\n",
      "    Round 1: 0.6748\n",
      "    Round 2: 0.6856\n",
      "    Round 3: 0.6942\n",
      "    Round 4: 0.6892\n",
      "    Round 5: 0.6861\n",
      "    Round 6: 0.6947\n",
      "    Round 7: 0.6835\n",
      "    Round 8: 0.6605\n",
      "    Round 9: 0.6915\n",
      "  Average: 0.6847\n",
      "Testing batch_size 10, lr=0.01, units=25\n",
      "    Round 0: 0.6672\n",
      "    Round 1: 0.6829\n",
      "    Round 2: 0.6845\n",
      "    Round 3: 0.6764\n",
      "    Round 4: 0.6725\n",
      "    Round 5: 0.6752\n",
      "    Round 6: 0.6958\n",
      "    Round 7: 0.6851\n",
      "    Round 8: 0.6886\n",
      "    Round 9: 0.6882\n",
      "  Average: 0.6817\n",
      "Testing batch_size 10, lr=0.01, units=50\n",
      "    Round 0: 0.6930\n",
      "    Round 1: 0.6907\n",
      "    Round 2: 0.6844\n",
      "    Round 3: 0.6916\n",
      "    Round 4: 0.6752\n",
      "    Round 5: 0.6900\n",
      "    Round 6: 0.6847\n",
      "    Round 7: 0.6888\n",
      "    Round 8: 0.6888\n",
      "    Round 9: 0.6821\n",
      "  Average: 0.6869\n",
      "Testing batch_size 10, lr=0.01, units=100\n",
      "    Round 0: 0.6911\n",
      "    Round 1: 0.6918\n",
      "    Round 2: 0.6898\n",
      "    Round 3: 0.6847\n",
      "    Round 4: 0.6834\n",
      "    Round 5: 0.6889\n",
      "    Round 6: 0.6815\n",
      "    Round 7: 0.6953\n",
      "    Round 8: 0.6940\n",
      "    Round 9: 0.6835\n",
      "  Average: 0.6884\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.6084\n",
      "    Round 1: 0.6519\n",
      "    Round 2: 0.6584\n",
      "    Round 3: 0.5034\n",
      "    Round 4: 0.5394\n",
      "    Round 5: 0.8077\n",
      "    Round 6: 0.7352\n",
      "    Round 7: 0.5293\n",
      "    Round 8: 0.7030\n",
      "    Round 9: 0.4388\n",
      "  Average: 0.6175\n",
      "Testing batch_size 25, lr=0.0001, units=25\n",
      "    Round 0: 0.5508\n",
      "    Round 1: 0.7310\n",
      "    Round 2: 0.5522\n",
      "    Round 3: 0.7926\n",
      "    Round 4: 0.5344\n",
      "    Round 5: 0.6256\n",
      "    Round 6: 0.6341\n",
      "    Round 7: 0.5535\n",
      "    Round 8: 0.6630\n",
      "    Round 9: 0.4735\n",
      "  Average: 0.6111\n",
      "Testing batch_size 25, lr=0.0001, units=50\n",
      "    Round 0: 0.6330\n",
      "    Round 1: 0.5879\n",
      "    Round 2: 0.6938\n",
      "    Round 3: 0.6861\n",
      "    Round 4: 0.7566\n",
      "    Round 5: 0.8393\n",
      "    Round 6: 0.5560\n",
      "    Round 7: 0.6640\n",
      "    Round 8: 0.8274\n",
      "    Round 9: 0.5151\n",
      "  Average: 0.6759\n",
      "Testing batch_size 25, lr=0.0001, units=100\n",
      "    Round 0: 0.5882\n",
      "    Round 1: 0.6727\n",
      "    Round 2: 0.8111\n",
      "    Round 3: 0.5637\n",
      "    Round 4: 0.8166\n",
      "    Round 5: 0.9475\n",
      "    Round 6: 0.6500\n",
      "    Round 7: 0.6710\n",
      "    Round 8: 0.5767\n",
      "    Round 9: 0.6400\n",
      "  Average: 0.6937\n",
      "Testing batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 1.4653\n",
      "    Round 1: 1.9864\n",
      "    Round 2: 2.3665\n",
      "    Round 3: 0.9654\n",
      "    Round 4: 1.2891\n",
      "    Round 5: 1.9980\n",
      "    Round 6: 1.4650\n",
      "    Round 7: 1.2970\n",
      "    Round 8: 1.1956\n",
      "    Round 9: 1.2101\n",
      "  Average: 1.5238\n",
      "Testing batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 1.6543\n",
      "    Round 1: 1.6852\n",
      "    Round 2: 1.2201\n",
      "    Round 3: 1.4811\n",
      "    Round 4: 1.3709\n",
      "    Round 5: 1.6760\n",
      "    Round 6: 1.1811\n",
      "    Round 7: 1.1635\n",
      "    Round 8: 1.2912\n",
      "    Round 9: 1.7017\n",
      "  Average: 1.4425\n",
      "Testing batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 1.8916\n",
      "    Round 1: 1.1328\n",
      "    Round 2: 1.5055\n",
      "    Round 3: 1.9602\n",
      "    Round 4: 1.8492\n",
      "    Round 5: 2.2053\n",
      "    Round 6: 1.6952\n",
      "    Round 7: 1.6390\n",
      "    Round 8: 1.8712\n",
      "    Round 9: 2.1958\n",
      "  Average: 1.7946\n",
      "Testing batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 1.8867\n",
      "    Round 1: 1.8382\n",
      "    Round 2: 1.5090\n",
      "    Round 3: 1.5063\n",
      "    Round 4: 1.2483\n",
      "    Round 5: 1.4324\n",
      "    Round 6: 1.9770\n",
      "    Round 7: 1.4850\n",
      "    Round 8: 2.1188\n",
      "    Round 9: 1.4967\n",
      "  Average: 1.6498\n",
      "Testing batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 1.3711\n",
      "    Round 1: 0.9788\n",
      "    Round 2: 0.6090\n",
      "    Round 3: 1.0728\n",
      "    Round 4: 1.3115\n",
      "    Round 5: 1.1643\n",
      "    Round 6: 1.4652\n",
      "    Round 7: 0.7653\n",
      "    Round 8: 0.6919\n",
      "    Round 9: 1.8559\n",
      "  Average: 1.1286\n",
      "Testing batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 1.2833\n",
      "    Round 1: 0.6254\n",
      "    Round 2: 1.4892\n",
      "    Round 3: 0.9901\n",
      "    Round 4: 0.7276\n",
      "    Round 5: 0.7565\n",
      "    Round 6: 0.7353\n",
      "    Round 7: 1.5106\n",
      "    Round 8: 0.8628\n",
      "    Round 9: 2.2474\n",
      "  Average: 1.1228\n",
      "Testing batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.7150\n",
      "    Round 1: 0.6843\n",
      "    Round 2: 1.9366\n",
      "    Round 3: 1.2559\n",
      "    Round 4: 0.8965\n",
      "    Round 5: 2.3766\n",
      "    Round 6: 0.6819\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [10, 25]\n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "batch_size, lr, hidden_units = model_tuning(\n",
    "    gen_baseline_2, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-liabilities",
   "metadata": {},
   "source": [
    "## Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_weight_sharing(params):\n",
    "    # No hidden layer units, ignore parameter\n",
    "    \n",
    "    model = models.WeightSharingCNN(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-tomorrow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = 10, 25\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "batch_size, lr, hidden_units = model_tuning(\n",
    "    gen_weight_sharing, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-immune",
   "metadata": {},
   "source": [
    "## Weight Sharing and Auxiliary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-small",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
