{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "written-trauma",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-stress",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "right-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Niels/Documents/EPFL/Master/DeepLearning/project1\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-deadline",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "traditional-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models as models\n",
    "from train import train\n",
    "from evaluation import model_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-science",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline(params):\n",
    "    # No hidden layer units, ignore hidden_units parameter\n",
    "    \n",
    "    model = models.BaselineCNN()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-cutting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 5, lr=0.0001, units=10\n",
      "    Round 0: 1.1092\n",
      "    Round 1: 0.7969\n",
      "    Round 2: 0.7165\n",
      "    Round 3: 0.7688\n",
      "    Round 4: 0.7141\n",
      "    Round 5: 0.6834\n",
      "    Round 6: 1.2128\n",
      "    Round 7: 0.7013\n",
      "    Round 8: 0.7184\n",
      "    Round 9: 0.9250\n",
      "  Average: 0.8346\n",
      "Testing batch_size 5, lr=0.001, units=10\n",
      "    Round 0: 1.8809\n",
      "    Round 1: 1.2092\n",
      "    Round 2: 2.2841\n",
      "    Round 3: 1.2975\n",
      "    Round 4: 1.4690\n",
      "    Round 5: 0.9899\n",
      "    Round 6: 3.1117\n",
      "    Round 7: 1.3108\n",
      "    Round 8: 1.3600\n",
      "    Round 9: 1.5018\n",
      "  Average: 1.6415\n",
      "Testing batch_size 5, lr=0.01, units=10\n",
      "    Round 0: 0.6871\n",
      "    Round 1: 0.6862\n",
      "    Round 2: 0.6938\n",
      "    Round 3: 0.6946\n",
      "    Round 4: 0.7018\n",
      "    Round 5: 0.6874\n",
      "    Round 6: 0.6833\n",
      "    Round 7: 0.6924\n",
      "    Round 8: 0.6842\n",
      "    Round 9: 0.6942\n",
      "  Average: 0.6905\n",
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.6710\n",
      "    Round 1: 0.4592\n",
      "    Round 2: 0.6989\n",
      "    Round 3: 0.5265\n",
      "    Round 4: 0.7384\n",
      "    Round 5: 0.8710\n",
      "    Round 6: 0.6416\n",
      "    Round 7: 0.6832\n",
      "    Round 8: 0.7944\n",
      "    Round 9: 0.7787\n",
      "  Average: 0.6863\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 1.7528\n",
      "    Round 1: 1.2634\n",
      "    Round 2: 1.5326\n",
      "    Round 3: 1.8392\n",
      "    Round 4: 1.5444\n",
      "    Round 5: 1.1966\n",
      "    Round 6: 1.5006\n",
      "    Round 7: 1.4620\n",
      "    Round 8: 1.2984\n",
      "    Round 9: 1.6435\n",
      "  Average: 1.5034\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6863\n",
      "    Round 1: 0.6228\n",
      "    Round 2: 0.6833\n",
      "    Round 3: 0.7196\n",
      "    Round 4: 0.6965\n",
      "    Round 5: 0.6871\n",
      "    Round 6: 0.6947\n",
      "    Round 7: 0.6861\n",
      "    Round 8: 1.1006\n",
      "    Round 9: 0.6927\n",
      "  Average: 0.7270\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.5113\n",
      "    Round 1: 0.6366\n",
      "    Round 2: 0.6295\n",
      "    Round 3: 0.5159\n",
      "    Round 4: 0.5121\n",
      "    Round 5: 0.5831\n",
      "    Round 6: 0.6528\n",
      "    Round 7: 0.6365\n",
      "    Round 8: 0.6296\n",
      "    Round 9: 0.5520\n",
      "  Average: 0.5859\n",
      "Testing batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 1.2979\n",
      "    Round 1: 1.2085\n",
      "    Round 2: 1.2856\n",
      "    Round 3: 1.5024\n",
      "    Round 4: 1.4923\n",
      "    Round 5: 1.3779\n",
      "    Round 6: 1.4220\n",
      "    Round 7: 1.0885\n",
      "    Round 8: 1.1858\n",
      "    Round 9: 1.2755\n",
      "  Average: 1.3137\n",
      "Testing batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.5239\n",
      "    Round 1: 1.3100\n",
      "    Round 2: 0.6686\n",
      "    Round 3: 1.1543\n",
      "    Round 4: 0.6781\n",
      "    Round 5: 1.4344\n",
      "    Round 6: 1.3751\n",
      "    Round 7: 1.8505\n",
      "    Round 8: 2.1428\n",
      "    Round 9: 0.6837\n",
      "  Average: 1.1821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 0.0001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [5, 10, 25]\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10]\n",
    "\n",
    "batch_size, lr, _ = model_tuning(\n",
    "    gen_baseline, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-answer",
   "metadata": {},
   "source": [
    "## Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline_2(params):\n",
    "    # No hidden layer units, ignore parameter\n",
    "    \n",
    "    model = models.BaselineCNN2(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-samoa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.7136\n",
      "    Round 1: 0.8627\n",
      "    Round 2: 0.7815\n",
      "    Round 3: 0.8811\n",
      "    Round 4: 0.7991\n",
      "    Round 5: 0.9208\n",
      "    Round 6: 1.0042\n",
      "    Round 7: 0.7632\n",
      "    Round 8: 0.9754\n",
      "    Round 9: 0.6918\n",
      "  Average: 0.8393\n",
      "Testing batch_size 10, lr=0.0001, units=25\n",
      "    Round 0: 0.9505\n",
      "    Round 1: 1.2050\n",
      "    Round 2: 1.0164\n",
      "    Round 3: 0.6881\n",
      "    Round 4: 1.2187\n",
      "    Round 5: 1.1406\n",
      "    Round 6: 0.9415\n",
      "    Round 7: 0.7762\n",
      "    Round 8: 0.7240\n",
      "    Round 9: 1.0407\n",
      "  Average: 0.9702\n",
      "Testing batch_size 10, lr=0.0001, units=50\n",
      "    Round 0: 1.3807\n",
      "    Round 1: 0.9928\n",
      "    Round 2: 0.9718\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [10, 25]\n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "batch_size, lr, hidden_units = model_tuning(\n",
    "    gen_baseline_2, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-liabilities",
   "metadata": {},
   "source": [
    "## Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_weight_sharing(params):\n",
    "    # No hidden layer units, ignore parameter\n",
    "    \n",
    "    model = models.WeightSharingCNN(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-tomorrow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = 10, 25\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "batch_size, lr, hidden_units = model_tuning(\n",
    "    gen_weight_sharing, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-immune",
   "metadata": {},
   "source": [
    "## Weight Sharing and Auxiliary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-small",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
