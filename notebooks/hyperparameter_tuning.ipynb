{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "written-trauma",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-stress",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "right-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Niels/Documents/EPFL/Master/DeepLearning/project1\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-deadline",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "traditional-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models as models\n",
    "from train import train\n",
    "from evaluation import model_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-science",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline(params):\n",
    "    # No hidden layer units, ignore hidden_units parameter\n",
    "    \n",
    "    model = models.BaselineCNN()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-cutting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 5, lr=0.0001, units=10\n",
      "    Round 0: 1.1092\n",
      "    Round 1: 0.7969\n",
      "    Round 2: 0.7165\n",
      "    Round 3: 0.7688\n",
      "    Round 4: 0.7141\n",
      "    Round 5: 0.6834\n",
      "    Round 6: 1.2128\n",
      "    Round 7: 0.7013\n",
      "    Round 8: 0.7184\n",
      "    Round 9: 0.9250\n",
      "  Average: 0.8346\n",
      "Testing batch_size 5, lr=0.001, units=10\n",
      "    Round 0: 1.8809\n",
      "    Round 1: 1.2092\n",
      "    Round 2: 2.2841\n",
      "    Round 3: 1.2975\n",
      "    Round 4: 1.4690\n",
      "    Round 5: 0.9899\n",
      "    Round 6: 3.1117\n",
      "    Round 7: 1.3108\n",
      "    Round 8: 1.3600\n",
      "    Round 9: 1.5018\n",
      "  Average: 1.6415\n",
      "Testing batch_size 5, lr=0.01, units=10\n",
      "    Round 0: 0.6871\n",
      "    Round 1: 0.6862\n",
      "    Round 2: 0.6938\n",
      "    Round 3: 0.6946\n",
      "    Round 4: 0.7018\n",
      "    Round 5: 0.6874\n",
      "    Round 6: 0.6833\n",
      "    Round 7: 0.6924\n",
      "    Round 8: 0.6842\n",
      "    Round 9: 0.6942\n",
      "  Average: 0.6905\n",
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.6710\n",
      "    Round 1: 0.4592\n",
      "    Round 2: 0.6989\n",
      "    Round 3: 0.5265\n",
      "    Round 4: 0.7384\n",
      "    Round 5: 0.8710\n",
      "    Round 6: 0.6416\n",
      "    Round 7: 0.6832\n",
      "    Round 8: 0.7944\n",
      "    Round 9: 0.7787\n",
      "  Average: 0.6863\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 1.7528\n",
      "    Round 1: 1.2634\n",
      "    Round 2: 1.5326\n",
      "    Round 3: 1.8392\n",
      "    Round 4: 1.5444\n",
      "    Round 5: 1.1966\n",
      "    Round 6: 1.5006\n",
      "    Round 7: 1.4620\n",
      "    Round 8: 1.2984\n",
      "    Round 9: 1.6435\n",
      "  Average: 1.5034\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6863\n",
      "    Round 1: 0.6228\n",
      "    Round 2: 0.6833\n",
      "    Round 3: 0.7196\n",
      "    Round 4: 0.6965\n",
      "    Round 5: 0.6871\n",
      "    Round 6: 0.6947\n",
      "    Round 7: 0.6861\n",
      "    Round 8: 1.1006\n",
      "    Round 9: 0.6927\n",
      "  Average: 0.7270\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.5113\n",
      "    Round 1: 0.6366\n",
      "    Round 2: 0.6295\n",
      "    Round 3: 0.5159\n",
      "    Round 4: 0.5121\n",
      "    Round 5: 0.5831\n",
      "    Round 6: 0.6528\n",
      "    Round 7: 0.6365\n",
      "    Round 8: 0.6296\n",
      "    Round 9: 0.5520\n",
      "  Average: 0.5859\n",
      "Testing batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 1.2979\n",
      "    Round 1: 1.2085\n",
      "    Round 2: 1.2856\n",
      "    Round 3: 1.5024\n",
      "    Round 4: 1.4923\n",
      "    Round 5: 1.3779\n",
      "    Round 6: 1.4220\n",
      "    Round 7: 1.0885\n",
      "    Round 8: 1.1858\n",
      "    Round 9: 1.2755\n",
      "  Average: 1.3137\n",
      "Testing batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.5239\n",
      "    Round 1: 1.3100\n",
      "    Round 2: 0.6686\n",
      "    Round 3: 1.1543\n",
      "    Round 4: 0.6781\n",
      "    Round 5: 1.4344\n",
      "    Round 6: 1.3751\n",
      "    Round 7: 1.8505\n",
      "    Round 8: 2.1428\n",
      "    Round 9: 0.6837\n",
      "  Average: 1.1821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 0.0001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [5, 10, 25]\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10]\n",
    "\n",
    "batch_size, lr, _ = model_tuning(\n",
    "    gen_baseline, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-answer",
   "metadata": {},
   "source": [
    "## Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_baseline_2(params):\n",
    "    # No hidden layer units, ignore parameter\n",
    "    \n",
    "    model = models.BaselineCNN2(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coastal-samoa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.7136\n",
      "    Round 1: 0.8627\n",
      "    Round 2: 0.7815\n",
      "    Round 3: 0.8811\n",
      "    Round 4: 0.7991\n",
      "    Round 5: 0.9208\n",
      "    Round 6: 1.0042\n",
      "    Round 7: 0.7632\n",
      "    Round 8: 0.9754\n",
      "    Round 9: 0.6918\n",
      "  Average: 0.8393\n",
      "Testing batch_size 10, lr=0.0001, units=25\n",
      "    Round 0: 0.9505\n",
      "    Round 1: 1.2050\n",
      "    Round 2: 1.0164\n",
      "    Round 3: 0.6881\n",
      "    Round 4: 1.2187\n",
      "    Round 5: 1.1406\n",
      "    Round 6: 0.9415\n",
      "    Round 7: 0.7762\n",
      "    Round 8: 0.7240\n",
      "    Round 9: 1.0407\n",
      "  Average: 0.9702\n",
      "Testing batch_size 10, lr=0.0001, units=50\n",
      "    Round 0: 1.3807\n",
      "    Round 1: 0.9928\n",
      "    Round 2: 0.9718\n",
      "    Round 3: 0.8047\n",
      "    Round 4: 0.9663\n",
      "    Round 5: 0.6732\n",
      "    Round 6: 0.6444\n",
      "    Round 7: 0.9131\n",
      "    Round 8: 1.0252\n",
      "    Round 9: 0.8202\n",
      "  Average: 0.9192\n",
      "Testing batch_size 10, lr=0.0001, units=100\n",
      "    Round 0: 0.8572\n",
      "    Round 1: 1.1305\n",
      "    Round 2: 1.2486\n",
      "    Round 3: 1.0130\n",
      "    Round 4: 0.9079\n",
      "    Round 5: 0.9733\n",
      "    Round 6: 0.9500\n",
      "    Round 7: 0.8811\n",
      "    Round 8: 0.6967\n",
      "    Round 9: 0.7953\n",
      "  Average: 0.9453\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 1.6573\n",
      "    Round 1: 2.5511\n",
      "    Round 2: 1.9330\n",
      "    Round 3: 1.3435\n",
      "    Round 4: 1.1243\n",
      "    Round 5: 1.7298\n",
      "    Round 6: 0.6900\n",
      "    Round 7: 1.9214\n",
      "    Round 8: 1.4959\n",
      "    Round 9: 0.6784\n",
      "  Average: 1.5125\n",
      "Testing batch_size 10, lr=0.001, units=25\n",
      "    Round 0: 1.5979\n",
      "    Round 1: 1.6433\n",
      "    Round 2: 1.7043\n",
      "    Round 3: 1.8209\n",
      "    Round 4: 1.5714\n",
      "    Round 5: 1.1264\n",
      "    Round 6: 1.3227\n",
      "    Round 7: 1.8581\n",
      "    Round 8: 1.3866\n",
      "    Round 9: 1.8087\n",
      "  Average: 1.5840\n",
      "Testing batch_size 10, lr=0.001, units=50\n",
      "    Round 0: 1.4713\n",
      "    Round 1: 1.7442\n",
      "    Round 2: 1.4576\n",
      "    Round 3: 1.4567\n",
      "    Round 4: 2.4276\n",
      "    Round 5: 1.6219\n",
      "    Round 6: 1.9430\n",
      "    Round 7: 2.3411\n",
      "    Round 8: 1.5257\n",
      "    Round 9: 1.4640\n",
      "  Average: 1.7453\n",
      "Testing batch_size 10, lr=0.001, units=100\n",
      "    Round 0: 1.7027\n",
      "    Round 1: 2.3635\n",
      "    Round 2: 1.5008\n",
      "    Round 3: 1.8197\n",
      "    Round 4: 1.3949\n",
      "    Round 5: 1.6832\n",
      "    Round 6: 1.7729\n",
      "    Round 7: 1.1171\n",
      "    Round 8: 1.3653\n",
      "    Round 9: 2.4515\n",
      "  Average: 1.7172\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6872\n",
      "    Round 1: 0.6748\n",
      "    Round 2: 0.6856\n",
      "    Round 3: 0.6942\n",
      "    Round 4: 0.6892\n",
      "    Round 5: 0.6861\n",
      "    Round 6: 0.6947\n",
      "    Round 7: 0.6835\n",
      "    Round 8: 0.6605\n",
      "    Round 9: 0.6915\n",
      "  Average: 0.6847\n",
      "Testing batch_size 10, lr=0.01, units=25\n",
      "    Round 0: 0.6672\n",
      "    Round 1: 0.6829\n",
      "    Round 2: 0.6845\n",
      "    Round 3: 0.6764\n",
      "    Round 4: 0.6725\n",
      "    Round 5: 0.6752\n",
      "    Round 6: 0.6958\n",
      "    Round 7: 0.6851\n",
      "    Round 8: 0.6886\n",
      "    Round 9: 0.6882\n",
      "  Average: 0.6817\n",
      "Testing batch_size 10, lr=0.01, units=50\n",
      "    Round 0: 0.6930\n",
      "    Round 1: 0.6907\n",
      "    Round 2: 0.6844\n",
      "    Round 3: 0.6916\n",
      "    Round 4: 0.6752\n",
      "    Round 5: 0.6900\n",
      "    Round 6: 0.6847\n",
      "    Round 7: 0.6888\n",
      "    Round 8: 0.6888\n",
      "    Round 9: 0.6821\n",
      "  Average: 0.6869\n",
      "Testing batch_size 10, lr=0.01, units=100\n",
      "    Round 0: 0.6911\n",
      "    Round 1: 0.6918\n",
      "    Round 2: 0.6898\n",
      "    Round 3: 0.6847\n",
      "    Round 4: 0.6834\n",
      "    Round 5: 0.6889\n",
      "    Round 6: 0.6815\n",
      "    Round 7: 0.6953\n",
      "    Round 8: 0.6940\n",
      "    Round 9: 0.6835\n",
      "  Average: 0.6884\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.6084\n",
      "    Round 1: 0.6519\n",
      "    Round 2: 0.6584\n",
      "    Round 3: 0.5034\n",
      "    Round 4: 0.5394\n",
      "    Round 5: 0.8077\n",
      "    Round 6: 0.7352\n",
      "    Round 7: 0.5293\n",
      "    Round 8: 0.7030\n",
      "    Round 9: 0.4388\n",
      "  Average: 0.6175\n",
      "Testing batch_size 25, lr=0.0001, units=25\n",
      "    Round 0: 0.5508\n",
      "    Round 1: 0.7310\n",
      "    Round 2: 0.5522\n",
      "    Round 3: 0.7926\n",
      "    Round 4: 0.5344\n",
      "    Round 5: 0.6256\n",
      "    Round 6: 0.6341\n",
      "    Round 7: 0.5535\n",
      "    Round 8: 0.6630\n",
      "    Round 9: 0.4735\n",
      "  Average: 0.6111\n",
      "Testing batch_size 25, lr=0.0001, units=50\n",
      "    Round 0: 0.6330\n",
      "    Round 1: 0.5879\n",
      "    Round 2: 0.6938\n",
      "    Round 3: 0.6861\n",
      "    Round 4: 0.7566\n",
      "    Round 5: 0.8393\n",
      "    Round 6: 0.5560\n",
      "    Round 7: 0.6640\n",
      "    Round 8: 0.8274\n",
      "    Round 9: 0.5151\n",
      "  Average: 0.6759\n",
      "Testing batch_size 25, lr=0.0001, units=100\n",
      "    Round 0: 0.5882\n",
      "    Round 1: 0.6727\n",
      "    Round 2: 0.8111\n",
      "    Round 3: 0.5637\n",
      "    Round 4: 0.8166\n",
      "    Round 5: 0.9475\n",
      "    Round 6: 0.6500\n",
      "    Round 7: 0.6710\n",
      "    Round 8: 0.5767\n",
      "    Round 9: 0.6400\n",
      "  Average: 0.6937\n",
      "Testing batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 1.4653\n",
      "    Round 1: 1.9864\n",
      "    Round 2: 2.3665\n",
      "    Round 3: 0.9654\n",
      "    Round 4: 1.2891\n",
      "    Round 5: 1.9980\n",
      "    Round 6: 1.4650\n",
      "    Round 7: 1.2970\n",
      "    Round 8: 1.1956\n",
      "    Round 9: 1.2101\n",
      "  Average: 1.5238\n",
      "Testing batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 1.6543\n",
      "    Round 1: 1.6852\n",
      "    Round 2: 1.2201\n",
      "    Round 3: 1.4811\n",
      "    Round 4: 1.3709\n",
      "    Round 5: 1.6760\n",
      "    Round 6: 1.1811\n",
      "    Round 7: 1.1635\n",
      "    Round 8: 1.2912\n",
      "    Round 9: 1.7017\n",
      "  Average: 1.4425\n",
      "Testing batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 1.8916\n",
      "    Round 1: 1.1328\n",
      "    Round 2: 1.5055\n",
      "    Round 3: 1.9602\n",
      "    Round 4: 1.8492\n",
      "    Round 5: 2.2053\n",
      "    Round 6: 1.6952\n",
      "    Round 7: 1.6390\n",
      "    Round 8: 1.8712\n",
      "    Round 9: 2.1958\n",
      "  Average: 1.7946\n",
      "Testing batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 1.8867\n",
      "    Round 1: 1.8382\n",
      "    Round 2: 1.5090\n",
      "    Round 3: 1.5063\n",
      "    Round 4: 1.2483\n",
      "    Round 5: 1.4324\n",
      "    Round 6: 1.9770\n",
      "    Round 7: 1.4850\n",
      "    Round 8: 2.1188\n",
      "    Round 9: 1.4967\n",
      "  Average: 1.6498\n",
      "Testing batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 1.3711\n",
      "    Round 1: 0.9788\n",
      "    Round 2: 0.6090\n",
      "    Round 3: 1.0728\n",
      "    Round 4: 1.3115\n",
      "    Round 5: 1.1643\n",
      "    Round 6: 1.4652\n",
      "    Round 7: 0.7653\n",
      "    Round 8: 0.6919\n",
      "    Round 9: 1.8559\n",
      "  Average: 1.1286\n",
      "Testing batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 1.2833\n",
      "    Round 1: 0.6254\n",
      "    Round 2: 1.4892\n",
      "    Round 3: 0.9901\n",
      "    Round 4: 0.7276\n",
      "    Round 5: 0.7565\n",
      "    Round 6: 0.7353\n",
      "    Round 7: 1.5106\n",
      "    Round 8: 0.8628\n",
      "    Round 9: 2.2474\n",
      "  Average: 1.1228\n",
      "Testing batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.7150\n",
      "    Round 1: 0.6843\n",
      "    Round 2: 1.9366\n",
      "    Round 3: 1.2559\n",
      "    Round 4: 0.8965\n",
      "    Round 5: 2.3766\n",
      "    Round 6: 0.6819\n",
      "    Round 7: 0.7034\n",
      "    Round 8: 0.7347\n",
      "    Round 9: 0.6484\n",
      "  Average: 1.0633\n",
      "Testing batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.7518\n",
      "    Round 1: 1.0151\n",
      "    Round 2: 1.6142\n",
      "    Round 3: 0.6956\n",
      "    Round 4: 1.2120\n",
      "    Round 5: 0.6904\n",
      "    Round 6: 0.6853\n",
      "    Round 7: 0.6872\n",
      "    Round 8: 1.2281\n",
      "    Round 9: 1.1152\n",
      "  Average: 0.9695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 0.0001, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [10, 25]\n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "batch_size, lr, hidden_units = model_tuning(\n",
    "    gen_baseline_2, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-liabilities",
   "metadata": {},
   "source": [
    "## Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "competent-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_weight_sharing(params):\n",
    "    # No hidden layer units, ignore parameter\n",
    "    \n",
    "    model = models.WeightSharingCNN(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "covered-tomorrow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch_size 10, lr=0.0001, units=10\n",
      "    Round 0: 0.7591\n",
      "    Round 1: 0.3877\n",
      "    Round 2: 0.8379\n",
      "    Round 3: 0.6004\n",
      "    Round 4: 0.5764\n",
      "    Round 5: 0.7948\n",
      "    Round 6: 0.6878\n",
      "    Round 7: 0.7023\n",
      "    Round 8: 0.5114\n",
      "    Round 9: 0.4330\n",
      "  Average: 0.6291\n",
      "Testing batch_size 10, lr=0.0001, units=25\n",
      "    Round 0: 0.4893\n",
      "    Round 1: 0.6177\n",
      "    Round 2: 0.6025\n",
      "    Round 3: 0.6476\n",
      "    Round 4: 0.8818\n",
      "    Round 5: 0.8530\n",
      "    Round 6: 0.5474\n",
      "    Round 7: 0.7395\n",
      "    Round 8: 0.7035\n",
      "    Round 9: 0.5998\n",
      "  Average: 0.6682\n",
      "Testing batch_size 10, lr=0.0001, units=50\n",
      "    Round 0: 0.5974\n",
      "    Round 1: 0.4296\n",
      "    Round 2: 0.7356\n",
      "    Round 3: 0.5215\n",
      "    Round 4: 0.6665\n",
      "    Round 5: 0.4150\n",
      "    Round 6: 0.8220\n",
      "    Round 7: 0.7235\n",
      "    Round 8: 0.7509\n",
      "    Round 9: 0.8333\n",
      "  Average: 0.6495\n",
      "Testing batch_size 10, lr=0.0001, units=100\n",
      "    Round 0: 0.6128\n",
      "    Round 1: 0.8416\n",
      "    Round 2: 0.6294\n",
      "    Round 3: 0.9614\n",
      "    Round 4: 0.8297\n",
      "    Round 5: 0.6966\n",
      "    Round 6: 0.5451\n",
      "    Round 7: 0.7425\n",
      "    Round 8: 0.8525\n",
      "    Round 9: 0.5958\n",
      "  Average: 0.7307\n",
      "Testing batch_size 10, lr=0.001, units=10\n",
      "    Round 0: 1.3535\n",
      "    Round 1: 1.3191\n",
      "    Round 2: 1.8037\n",
      "    Round 3: 1.5329\n",
      "    Round 4: 1.1866\n",
      "    Round 5: 1.1904\n",
      "    Round 6: 0.8127\n",
      "    Round 7: 1.2788\n",
      "    Round 8: 1.0165\n",
      "    Round 9: 0.6128\n",
      "  Average: 1.2107\n",
      "Testing batch_size 10, lr=0.001, units=25\n",
      "    Round 0: 1.7290\n",
      "    Round 1: 1.7855\n",
      "    Round 2: 0.9083\n",
      "    Round 3: 1.4890\n",
      "    Round 4: 1.4690\n",
      "    Round 5: 1.1120\n",
      "    Round 6: 1.5525\n",
      "    Round 7: 1.2471\n",
      "    Round 8: 0.7736\n",
      "    Round 9: 1.2055\n",
      "  Average: 1.3271\n",
      "Testing batch_size 10, lr=0.001, units=50\n",
      "    Round 0: 1.1755\n",
      "    Round 1: 0.9367\n",
      "    Round 2: 0.8758\n",
      "    Round 3: 1.4383\n",
      "    Round 4: 1.4036\n",
      "    Round 5: 1.3128\n",
      "    Round 6: 0.7607\n",
      "    Round 7: 1.1223\n",
      "    Round 8: 1.2842\n",
      "    Round 9: 1.3976\n",
      "  Average: 1.1707\n",
      "Testing batch_size 10, lr=0.001, units=100\n",
      "    Round 0: 0.8012\n",
      "    Round 1: 1.1855\n",
      "    Round 2: 1.0299\n",
      "    Round 3: 0.6959\n",
      "    Round 4: 1.8314\n",
      "    Round 5: 1.2262\n",
      "    Round 6: 1.3948\n",
      "    Round 7: 1.7139\n",
      "    Round 8: 1.6550\n",
      "    Round 9: 1.3432\n",
      "  Average: 1.2877\n",
      "Testing batch_size 10, lr=0.01, units=10\n",
      "    Round 0: 0.6937\n",
      "    Round 1: 0.6035\n",
      "    Round 2: 0.6949\n",
      "    Round 3: 0.6775\n",
      "    Round 4: 0.6936\n",
      "    Round 5: 0.6765\n",
      "    Round 6: 0.6984\n",
      "    Round 7: 0.6867\n",
      "    Round 8: 0.6789\n",
      "    Round 9: 0.4798\n",
      "  Average: 0.6584\n",
      "Testing batch_size 10, lr=0.01, units=25\n",
      "    Round 0: 0.6804\n",
      "    Round 1: 0.6541\n",
      "    Round 2: 0.6922\n",
      "    Round 3: 0.6569\n",
      "    Round 4: 0.5118\n",
      "    Round 5: 0.7003\n",
      "    Round 6: 0.4317\n",
      "    Round 7: 0.6947\n",
      "    Round 8: 0.7006\n",
      "    Round 9: 0.5472\n",
      "  Average: 0.6270\n",
      "Testing batch_size 10, lr=0.01, units=50\n",
      "    Round 0: 0.6854\n",
      "    Round 1: 0.6805\n",
      "    Round 2: 0.6864\n",
      "    Round 3: 0.6767\n",
      "    Round 4: 0.6936\n",
      "    Round 5: 0.6091\n",
      "    Round 6: 0.6037\n",
      "    Round 7: 0.6347\n",
      "    Round 8: 0.6891\n",
      "    Round 9: 0.6954\n",
      "  Average: 0.6655\n",
      "Testing batch_size 10, lr=0.01, units=100\n",
      "    Round 0: 0.7663\n",
      "    Round 1: 0.6921\n",
      "    Round 2: 0.6936\n",
      "    Round 3: 0.4773\n",
      "    Round 4: 0.7436\n",
      "    Round 5: 0.4654\n",
      "    Round 6: 0.4857\n",
      "    Round 7: 0.6893\n",
      "    Round 8: 0.6915\n",
      "    Round 9: 0.6832\n",
      "  Average: 0.6388\n",
      "Testing batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.4176\n",
      "    Round 1: 0.2317\n",
      "    Round 2: 0.4651\n",
      "    Round 3: 0.4079\n",
      "    Round 4: 0.4610\n",
      "    Round 5: 0.5578\n",
      "    Round 6: 0.5502\n",
      "    Round 7: 0.6174\n",
      "    Round 8: 0.6722\n",
      "    Round 9: 0.4787\n",
      "  Average: 0.4860\n",
      "Testing batch_size 25, lr=0.0001, units=25\n",
      "    Round 0: 0.4149\n",
      "    Round 1: 0.7220\n",
      "    Round 2: 0.4549\n",
      "    Round 3: 0.4489\n",
      "    Round 4: 0.4994\n",
      "    Round 5: 0.6576\n",
      "    Round 6: 0.6448\n",
      "    Round 7: 0.6275\n",
      "    Round 8: 0.6300\n",
      "    Round 9: 0.5870\n",
      "  Average: 0.5687\n",
      "Testing batch_size 25, lr=0.0001, units=50\n",
      "    Round 0: 0.4638\n",
      "    Round 1: 0.4160\n",
      "    Round 2: 0.4650\n",
      "    Round 3: 0.4852\n",
      "    Round 4: 0.4865\n",
      "    Round 5: 0.6857\n",
      "    Round 6: 0.5495\n",
      "    Round 7: 0.5388\n",
      "    Round 8: 0.5422\n",
      "    Round 9: 0.4612\n",
      "  Average: 0.5094\n",
      "Testing batch_size 25, lr=0.0001, units=100\n",
      "    Round 0: 0.6322\n",
      "    Round 1: 0.5319\n",
      "    Round 2: 0.4799\n",
      "    Round 3: 0.5157\n",
      "    Round 4: 0.6813\n",
      "    Round 5: 0.5129\n",
      "    Round 6: 0.5970\n",
      "    Round 7: 0.3786\n",
      "    Round 8: 0.5672\n",
      "    Round 9: 0.6777\n",
      "  Average: 0.5574\n",
      "Testing batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 0.9722\n",
      "    Round 1: 1.4541\n",
      "    Round 2: 0.9903\n",
      "    Round 3: 1.1665\n",
      "    Round 4: 1.1645\n",
      "    Round 5: 1.0435\n",
      "    Round 6: 1.4147\n",
      "    Round 7: 1.0189\n",
      "    Round 8: 1.2602\n",
      "    Round 9: 1.1556\n",
      "  Average: 1.1641\n",
      "Testing batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 1.0465\n",
      "    Round 1: 0.9878\n",
      "    Round 2: 0.9057\n",
      "    Round 3: 1.5334\n",
      "    Round 4: 1.2122\n",
      "    Round 5: 1.7108\n",
      "    Round 6: 0.8114\n",
      "    Round 7: 0.8101\n",
      "    Round 8: 1.7598\n",
      "    Round 9: 1.3539\n",
      "  Average: 1.2132\n",
      "Testing batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 0.9474\n",
      "    Round 1: 0.7874\n",
      "    Round 2: 1.2901\n",
      "    Round 3: 1.4113\n",
      "    Round 4: 0.8893\n",
      "    Round 5: 1.3417\n",
      "    Round 6: 0.9973\n",
      "    Round 7: 1.6294\n",
      "    Round 8: 1.3028\n",
      "    Round 9: 1.2557\n",
      "  Average: 1.1852\n",
      "Testing batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 1.1933\n",
      "    Round 1: 0.8923\n",
      "    Round 2: 0.7076\n",
      "    Round 3: 1.1659\n",
      "    Round 4: 0.9343\n",
      "    Round 5: 0.7319\n",
      "    Round 6: 1.1132\n",
      "    Round 7: 1.1337\n",
      "    Round 8: 1.5239\n",
      "    Round 9: 1.4569\n",
      "  Average: 1.0853\n",
      "Testing batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.7331\n",
      "    Round 1: 0.4636\n",
      "    Round 2: 0.6909\n",
      "    Round 3: 0.8681\n",
      "    Round 4: 0.4108\n",
      "    Round 5: 0.7628\n",
      "    Round 6: 0.4881\n",
      "    Round 7: 0.6868\n",
      "    Round 8: 0.6897\n",
      "    Round 9: 0.6418\n",
      "  Average: 0.6436\n",
      "Testing batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 1.0280\n",
      "    Round 1: 0.6009\n",
      "    Round 2: 0.5530\n",
      "    Round 3: 0.6802\n",
      "    Round 4: 0.8623\n",
      "    Round 5: 0.7112\n",
      "    Round 6: 0.3985\n",
      "    Round 7: 0.6879\n",
      "    Round 8: 0.7138\n",
      "    Round 9: 0.3581\n",
      "  Average: 0.6594\n",
      "Testing batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.9745\n",
      "    Round 1: 0.6926\n",
      "    Round 2: 0.6607\n",
      "    Round 3: 1.2192\n",
      "    Round 4: 0.8325\n",
      "    Round 5: 0.5183\n",
      "    Round 6: 0.4264\n",
      "    Round 7: 0.6017\n",
      "    Round 8: 0.4411\n",
      "    Round 9: 0.6768\n",
      "  Average: 0.7044\n",
      "Testing batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.5846\n",
      "    Round 1: 0.4670\n",
      "    Round 2: 0.4719\n",
      "    Round 3: 0.4147\n",
      "    Round 4: 0.4907\n",
      "    Round 5: 0.7120\n",
      "    Round 6: 0.6360\n",
      "    Round 7: 0.9014\n",
      "    Round 8: 0.8052\n",
      "    Round 9: 0.6786\n",
      "  Average: 0.6162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 0.0001, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = 10, 25\n",
    "# Doesn't matter: no MLP after LeNet \n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "batch_size, lr, hidden_units = model_tuning(\n",
    "    gen_weight_sharing, train, epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, seed=seed)\n",
    "\n",
    "batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-immune",
   "metadata": {},
   "source": [
    "## Weight Sharing and Auxiliary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "major-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_with_auxiliary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standing-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_weight_sharing_aux_loss(params):\n",
    "    \"\"\" Generates the first baseline \"\"\"\n",
    "    model = models.WeightSharingAuxLossCNN(hidden_layer_units=params['hidden_units'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    aux_criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    return model, criterion, aux_criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-radical",
   "metadata": {},
   "source": [
    "### Aux Loss Weight 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "herbal-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import model_tuning_aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hollywood-scholar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing aux_weight 0.1, batch_size 25, lr=0.0001, units=10\n",
      "    Round 0: 0.1950\n",
      "    Round 1: 0.1200\n",
      "    Round 2: 0.1150\n",
      "    Round 3: 0.1800\n",
      "    Round 4: 0.1850\n",
      "    Round 5: 0.1850\n",
      "    Round 6: 0.1350\n",
      "    Round 7: 0.1850\n",
      "    Round 8: 0.1900\n",
      "    Round 9: 0.2050\n",
      "  Average: 0.1695\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.0001, units=25\n",
      "    Round 0: 0.1800\n",
      "    Round 1: 0.1750\n",
      "    Round 2: 0.1700\n",
      "    Round 3: 0.1450\n",
      "    Round 4: 0.1850\n",
      "    Round 5: 0.1750\n",
      "    Round 6: 0.1600\n",
      "    Round 7: 0.1800\n",
      "    Round 8: 0.1500\n",
      "    Round 9: 0.1400\n",
      "  Average: 0.1660\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.0001, units=50\n",
      "    Round 0: 0.2000\n",
      "    Round 1: 0.1600\n",
      "    Round 2: 0.1100\n",
      "    Round 3: 0.1250\n",
      "    Round 4: 0.1750\n",
      "    Round 5: 0.1650\n",
      "    Round 6: 0.1900\n",
      "    Round 7: 0.2200\n",
      "    Round 8: 0.2350\n",
      "    Round 9: 0.1100\n",
      "  Average: 0.1690\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.0001, units=100\n",
      "    Round 0: 0.1800\n",
      "    Round 1: 0.2050\n",
      "    Round 2: 0.1600\n",
      "    Round 3: 0.1600\n",
      "    Round 4: 0.1450\n",
      "    Round 5: 0.2150\n",
      "    Round 6: 0.1650\n",
      "    Round 7: 0.1350\n",
      "    Round 8: 0.1350\n",
      "    Round 9: 0.1550\n",
      "  Average: 0.1655\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 0.1400\n",
      "    Round 1: 0.1450\n",
      "    Round 2: 0.1700\n",
      "    Round 3: 0.1150\n",
      "    Round 4: 0.1000\n",
      "    Round 5: 0.1550\n",
      "    Round 6: 0.1650\n",
      "    Round 7: 0.1150\n",
      "    Round 8: 0.1700\n",
      "    Round 9: 0.1550\n",
      "  Average: 0.1430\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 0.1100\n",
      "    Round 1: 0.0950\n",
      "    Round 2: 0.1000\n",
      "    Round 3: 0.1200\n",
      "    Round 4: 0.1400\n",
      "    Round 5: 0.1400\n",
      "    Round 6: 0.1450\n",
      "    Round 7: 0.1300\n",
      "    Round 8: 0.1150\n",
      "    Round 9: 0.1250\n",
      "  Average: 0.1220\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 0.1200\n",
      "    Round 1: 0.0750\n",
      "    Round 2: 0.1600\n",
      "    Round 3: 0.1000\n",
      "    Round 4: 0.1300\n",
      "    Round 5: 0.0950\n",
      "    Round 6: 0.0950\n",
      "    Round 7: 0.1650\n",
      "    Round 8: 0.1600\n",
      "    Round 9: 0.1000\n",
      "  Average: 0.1200\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 0.1550\n",
      "    Round 1: 0.1100\n",
      "    Round 2: 0.1700\n",
      "    Round 3: 0.1500\n",
      "    Round 4: 0.1100\n",
      "    Round 5: 0.1350\n",
      "    Round 6: 0.1250\n",
      "    Round 7: 0.0750\n",
      "    Round 8: 0.1300\n",
      "    Round 9: 0.0850\n",
      "  Average: 0.1245\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.1850\n",
      "    Round 1: 0.1000\n",
      "    Round 2: 0.1800\n",
      "    Round 3: 0.1700\n",
      "    Round 4: 0.1400\n",
      "    Round 5: 0.1400\n",
      "    Round 6: 0.1450\n",
      "    Round 7: 0.2050\n",
      "    Round 8: 0.1550\n",
      "    Round 9: 0.1250\n",
      "  Average: 0.1545\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 0.1550\n",
      "    Round 1: 0.2500\n",
      "    Round 2: 0.1200\n",
      "    Round 3: 0.2050\n",
      "    Round 4: 0.1300\n",
      "    Round 5: 0.2500\n",
      "    Round 6: 0.1750\n",
      "    Round 7: 0.1900\n",
      "    Round 8: 0.1700\n",
      "    Round 9: 0.1500\n",
      "  Average: 0.1795\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.2750\n",
      "    Round 1: 0.1350\n",
      "    Round 2: 0.1500\n",
      "    Round 3: 0.1200\n",
      "    Round 4: 0.1850\n",
      "    Round 5: 0.2000\n",
      "    Round 6: 0.1750\n",
      "    Round 7: 0.1650\n",
      "    Round 8: 0.1350\n",
      "    Round 9: 0.1950\n",
      "  Average: 0.1735\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.2200\n",
      "    Round 1: 0.1600\n",
      "    Round 2: 0.1900\n",
      "    Round 3: 0.1700\n",
      "    Round 4: 0.1350\n",
      "    Round 5: 0.1400\n",
      "    Round 6: 0.1900\n",
      "    Round 7: 0.1500\n",
      "    Round 8: 0.0850\n",
      "    Round 9: 0.4450\n",
      "  Average: 0.1885\n",
      "Testing aux_weight 0.1, batch_size 50, lr=0.0001, units=10\n",
      "    Round 0: 0.1650\n",
      "    Round 1: 0.2200\n",
      "    Round 2: 0.2200\n",
      "    Round 3: 0.1600\n",
      "    Round 4: 0.1550\n",
      "    Round 5: 0.2400\n",
      "    Round 6: 0.1900\n",
      "    Round 7: 0.1450\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.1650\n",
      "  Average: 0.1800\n",
      "Testing aux_weight 0.1, batch_size 50, lr=0.0001, units=25\n",
      "    Round 0: 0.1900\n",
      "    Round 1: 0.1850\n",
      "    Round 2: 0.1850\n",
      "    Round 3: 0.2050\n",
      "    Round 4: 0.2000\n",
      "    Round 5: 0.2050\n",
      "    Round 6: 0.1350\n",
      "    Round 7: 0.2100\n",
      "    Round 8: 0.1550\n",
      "    Round 9: 0.1600\n",
      "  Average: 0.1830\n",
      "Testing aux_weight 0.1, batch_size 50, lr=0.0001, units=50\n",
      "    Round 0: 0.1650\n",
      "    Round 1: 0.2050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-487fabe45ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mhidden_layer_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m aux_loss_weight, batch_size, lr, hidden_units = model_tuning_aux_loss(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mgen_weight_sharing_aux_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_with_auxiliary_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     batch_sizes, learning_rates, hidden_layer_units, aux_loss_weights, seed=seed)\n",
      "\u001b[0;32m~/Documents/EPFL/Master/DeepLearning/project1/evaluation.py\u001b[0m in \u001b[0;36mmodel_tuning_aux_loss\u001b[0;34m(gen_model, train_method, num_epochs, rounds, batch_sizes, learning_rates, hidden_layer_units, aux_loss_weights, seed, print_round_results)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         _, _, _, val_error = train_method(model, optimizer, criterion, aux_criterion, train_data,\n\u001b[0m\u001b[1;32m    179\u001b[0m                                                           val_data, num_epochs, aux_loss_weight=aux_weight)\n\u001b[1;32m    180\u001b[0m                         \u001b[0mfinal_val_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/DeepLearning/project1/train.py\u001b[0m in \u001b[0;36mtrain_with_auxiliary_loss\u001b[0;34m(model, optimizer, criterion, aux_criterion, train_data, test_data, num_epochs, aux_loss_weight)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# Compute the error rate on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_with_auxiliary_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_loss_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtrain_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/DeepLearning/project1/train.py\u001b[0m in \u001b[0;36mtest_with_auxiliary_loss\u001b[0;34m(model, criterion, aux_criterion, aux_loss_weight, dataloader)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_aux2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/DeepLearning/project1/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Process first images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Process second images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master/DeepLearning/project1/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "aux_loss_weights = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [25, 50]\n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "aux_loss_weight, batch_size, lr, hidden_units = model_tuning_aux_loss(\n",
    "    gen_weight_sharing_aux_loss, train_with_auxiliary_loss, epochs, rounds,\n",
    "    batch_sizes, learning_rates, hidden_layer_units, aux_loss_weights, seed=seed)\n",
    "\n",
    "aux_loss_weight, batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seasonal-visiting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing aux_weight 0.1, batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 0.1400\n",
      "    Round 1: 0.1300\n",
      "    Round 2: 0.1250\n",
      "    Round 3: 0.1450\n",
      "    Round 4: 0.1250\n",
      "    Round 5: 0.1050\n",
      "    Round 6: 0.0950\n",
      "    Round 7: 0.1350\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.1400\n",
      "  Average: 0.1280\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 0.1200\n",
      "    Round 1: 0.1600\n",
      "    Round 2: 0.1600\n",
      "    Round 3: 0.1100\n",
      "    Round 4: 0.1150\n",
      "    Round 5: 0.1250\n",
      "    Round 6: 0.1550\n",
      "    Round 7: 0.1550\n",
      "    Round 8: 0.1300\n",
      "    Round 9: 0.1150\n",
      "  Average: 0.1345\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 0.1250\n",
      "    Round 1: 0.1600\n",
      "    Round 2: 0.0750\n",
      "    Round 3: 0.0950\n",
      "    Round 4: 0.1050\n",
      "    Round 5: 0.1100\n",
      "    Round 6: 0.1600\n",
      "    Round 7: 0.1850\n",
      "    Round 8: 0.0850\n",
      "    Round 9: 0.1500\n",
      "  Average: 0.1250\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 0.1200\n",
      "    Round 1: 0.1300\n",
      "    Round 2: 0.1550\n",
      "    Round 3: 0.0900\n",
      "    Round 4: 0.1100\n",
      "    Round 5: 0.1150\n",
      "    Round 6: 0.1000\n",
      "    Round 7: 0.1350\n",
      "    Round 8: 0.1150\n",
      "    Round 9: 0.1150\n",
      "  Average: 0.1185\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.1250\n",
      "    Round 1: 0.0850\n",
      "    Round 2: 0.1350\n",
      "    Round 3: 0.1500\n",
      "    Round 4: 0.1350\n",
      "    Round 5: 0.1400\n",
      "    Round 6: 0.1650\n",
      "    Round 7: 0.0800\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.2400\n",
      "  Average: 0.1395\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 0.1800\n",
      "    Round 1: 0.2300\n",
      "    Round 2: 0.1900\n",
      "    Round 3: 0.2900\n",
      "    Round 4: 0.2100\n",
      "    Round 5: 0.1350\n",
      "    Round 6: 0.2000\n",
      "    Round 7: 0.1550\n",
      "    Round 8: 0.2000\n",
      "    Round 9: 0.1400\n",
      "  Average: 0.1930\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.1600\n",
      "    Round 1: 0.0750\n",
      "    Round 2: 0.1250\n",
      "    Round 3: 0.1400\n",
      "    Round 4: 0.1900\n",
      "    Round 5: 0.1150\n",
      "    Round 6: 0.1800\n",
      "    Round 7: 0.1400\n",
      "    Round 8: 0.2400\n",
      "    Round 9: 0.1500\n",
      "  Average: 0.1515\n",
      "Testing aux_weight 0.1, batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.1250\n",
      "    Round 1: 0.1650\n",
      "    Round 2: 0.2000\n",
      "    Round 3: 0.1150\n",
      "    Round 4: 0.1900\n",
      "    Round 5: 0.1350\n",
      "    Round 6: 0.1100\n",
      "    Round 7: 0.1600\n",
      "    Round 8: 0.1150\n",
      "    Round 9: 0.2500\n",
      "  Average: 0.1565\n",
      "Testing aux_weight 0.5, batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 0.1300\n",
      "    Round 1: 0.1650\n",
      "    Round 2: 0.1050\n",
      "    Round 3: 0.1000\n",
      "    Round 4: 0.1100\n",
      "    Round 5: 0.1200\n",
      "    Round 6: 0.1350\n",
      "    Round 7: 0.1500\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.1000\n",
      "  Average: 0.1255\n",
      "Testing aux_weight 0.5, batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 0.1250\n",
      "    Round 1: 0.1100\n",
      "    Round 2: 0.0900\n",
      "    Round 3: 0.1000\n",
      "    Round 4: 0.1550\n",
      "    Round 5: 0.1500\n",
      "    Round 6: 0.1000\n",
      "    Round 7: 0.1400\n",
      "    Round 8: 0.1550\n",
      "    Round 9: 0.1500\n",
      "  Average: 0.1275\n",
      "Testing aux_weight 0.5, batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 0.1150\n",
      "    Round 1: 0.1000\n",
      "    Round 2: 0.1050\n",
      "    Round 3: 0.0900\n",
      "    Round 4: 0.1350\n",
      "    Round 5: 0.0500\n",
      "    Round 6: 0.1250\n",
      "    Round 7: 0.1100\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.1250\n",
      "  Average: 0.1095\n",
      "Testing aux_weight 0.5, batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 0.1250\n",
      "    Round 1: 0.1250\n",
      "    Round 2: 0.1050\n",
      "    Round 3: 0.0900\n",
      "    Round 4: 0.0950\n",
      "    Round 5: 0.0950\n",
      "    Round 6: 0.1000\n",
      "    Round 7: 0.1100\n",
      "    Round 8: 0.1050\n",
      "    Round 9: 0.1350\n",
      "  Average: 0.1085\n",
      "Testing aux_weight 0.5, batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.1050\n",
      "    Round 1: 0.1600\n",
      "    Round 2: 0.1200\n",
      "    Round 3: 0.0750\n",
      "    Round 4: 0.1200\n",
      "    Round 5: 0.1500\n",
      "    Round 6: 0.1000\n",
      "    Round 7: 0.2350\n",
      "    Round 8: 0.1850\n",
      "    Round 9: 0.1450\n",
      "  Average: 0.1395\n",
      "Testing aux_weight 0.5, batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 0.1050\n",
      "    Round 1: 0.0950\n",
      "    Round 2: 0.1300\n",
      "    Round 3: 0.3800\n",
      "    Round 4: 0.1350\n",
      "    Round 5: 0.2100\n",
      "    Round 6: 0.1100\n",
      "    Round 7: 0.1000\n",
      "    Round 8: 0.0800\n",
      "    Round 9: 0.0750\n",
      "  Average: 0.1420\n",
      "Testing aux_weight 0.5, batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.2350\n",
      "    Round 1: 0.1600\n",
      "    Round 2: 0.1250\n",
      "    Round 3: 0.0950\n",
      "    Round 4: 0.1050\n",
      "    Round 5: 0.1650\n",
      "    Round 6: 0.1350\n",
      "    Round 7: 0.1050\n",
      "    Round 8: 0.0900\n",
      "    Round 9: 0.1500\n",
      "  Average: 0.1365\n",
      "Testing aux_weight 0.5, batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.1150\n",
      "    Round 1: 0.0950\n",
      "    Round 2: 0.0950\n",
      "    Round 3: 0.1450\n",
      "    Round 4: 0.1100\n",
      "    Round 5: 0.1000\n",
      "    Round 6: 0.1450\n",
      "    Round 7: 0.1900\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.1050\n",
      "  Average: 0.1240\n",
      "Testing aux_weight 1.0, batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 0.1200\n",
      "    Round 1: 0.1500\n",
      "    Round 2: 0.1350\n",
      "    Round 3: 0.1100\n",
      "    Round 4: 0.1250\n",
      "    Round 5: 0.1300\n",
      "    Round 6: 0.1050\n",
      "    Round 7: 0.1150\n",
      "    Round 8: 0.1200\n",
      "    Round 9: 0.1400\n",
      "  Average: 0.1250\n",
      "Testing aux_weight 1.0, batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 0.0900\n",
      "    Round 1: 0.1500\n",
      "    Round 2: 0.1450\n",
      "    Round 3: 0.1500\n",
      "    Round 4: 0.1000\n",
      "    Round 5: 0.1500\n",
      "    Round 6: 0.1650\n",
      "    Round 7: 0.1150\n",
      "    Round 8: 0.1550\n",
      "    Round 9: 0.0950\n",
      "  Average: 0.1315\n",
      "Testing aux_weight 1.0, batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 0.1450\n",
      "    Round 1: 0.0900\n",
      "    Round 2: 0.0900\n",
      "    Round 3: 0.0800\n",
      "    Round 4: 0.1200\n",
      "    Round 5: 0.0950\n",
      "    Round 6: 0.1050\n",
      "    Round 7: 0.1200\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.0950\n",
      "  Average: 0.1080\n",
      "Testing aux_weight 1.0, batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 0.0850\n",
      "    Round 1: 0.1200\n",
      "    Round 2: 0.0900\n",
      "    Round 3: 0.1400\n",
      "    Round 4: 0.1050\n",
      "    Round 5: 0.1100\n",
      "    Round 6: 0.1200\n",
      "    Round 7: 0.0650\n",
      "    Round 8: 0.1300\n",
      "    Round 9: 0.0900\n",
      "  Average: 0.1055\n",
      "Testing aux_weight 1.0, batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.1250\n",
      "    Round 1: 0.1150\n",
      "    Round 2: 0.1400\n",
      "    Round 3: 0.1550\n",
      "    Round 4: 0.1500\n",
      "    Round 5: 0.1350\n",
      "    Round 6: 0.1300\n",
      "    Round 7: 0.1400\n",
      "    Round 8: 0.1500\n",
      "    Round 9: 0.1200\n",
      "  Average: 0.1360\n",
      "Testing aux_weight 1.0, batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 0.1400\n",
      "    Round 1: 0.0650\n",
      "    Round 2: 0.0850\n",
      "    Round 3: 0.1100\n",
      "    Round 4: 0.2000\n",
      "    Round 5: 0.0650\n",
      "    Round 6: 0.0900\n",
      "    Round 7: 0.1100\n",
      "    Round 8: 0.0700\n",
      "    Round 9: 0.0800\n",
      "  Average: 0.1015\n",
      "Testing aux_weight 1.0, batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.1250\n",
      "    Round 1: 0.0850\n",
      "    Round 2: 0.1200\n",
      "    Round 3: 0.1050\n",
      "    Round 4: 0.1500\n",
      "    Round 5: 0.1150\n",
      "    Round 6: 0.0650\n",
      "    Round 7: 0.0900\n",
      "    Round 8: 0.1050\n",
      "    Round 9: 0.1200\n",
      "  Average: 0.1080\n",
      "Testing aux_weight 1.0, batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.0700\n",
      "    Round 1: 0.1300\n",
      "    Round 2: 0.1350\n",
      "    Round 3: 0.1050\n",
      "    Round 4: 0.0800\n",
      "    Round 5: 0.1650\n",
      "    Round 6: 0.1400\n",
      "    Round 7: 0.1200\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.4050\n",
      "  Average: 0.1490\n",
      "Testing aux_weight 2.0, batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 0.0650\n",
      "    Round 1: 0.0950\n",
      "    Round 2: 0.0850\n",
      "    Round 3: 0.1100\n",
      "    Round 4: 0.1100\n",
      "    Round 5: 0.1400\n",
      "    Round 6: 0.0800\n",
      "    Round 7: 0.1400\n",
      "    Round 8: 0.1300\n",
      "    Round 9: 0.1000\n",
      "  Average: 0.1055\n",
      "Testing aux_weight 2.0, batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 0.1250\n",
      "    Round 1: 0.1850\n",
      "    Round 2: 0.0950\n",
      "    Round 3: 0.1400\n",
      "    Round 4: 0.1200\n",
      "    Round 5: 0.0900\n",
      "    Round 6: 0.0950\n",
      "    Round 7: 0.1100\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.1300\n",
      "  Average: 0.1230\n",
      "Testing aux_weight 2.0, batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 0.1150\n",
      "    Round 1: 0.0900\n",
      "    Round 2: 0.1150\n",
      "    Round 3: 0.1350\n",
      "    Round 4: 0.1100\n",
      "    Round 5: 0.1000\n",
      "    Round 6: 0.0750\n",
      "    Round 7: 0.0900\n",
      "    Round 8: 0.1600\n",
      "    Round 9: 0.1050\n",
      "  Average: 0.1095\n",
      "Testing aux_weight 2.0, batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 0.1350\n",
      "    Round 1: 0.1000\n",
      "    Round 2: 0.0950\n",
      "    Round 3: 0.0650\n",
      "    Round 4: 0.0650\n",
      "    Round 5: 0.1400\n",
      "    Round 6: 0.1350\n",
      "    Round 7: 0.0650\n",
      "    Round 8: 0.1250\n",
      "    Round 9: 0.1100\n",
      "  Average: 0.1035\n",
      "Testing aux_weight 2.0, batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.1550\n",
      "    Round 1: 0.0800\n",
      "    Round 2: 0.1050\n",
      "    Round 3: 0.0800\n",
      "    Round 4: 0.1000\n",
      "    Round 5: 0.1450\n",
      "    Round 6: 0.0850\n",
      "    Round 7: 0.1450\n",
      "    Round 8: 0.1200\n",
      "    Round 9: 0.1650\n",
      "  Average: 0.1180\n",
      "Testing aux_weight 2.0, batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 0.1550\n",
      "    Round 1: 0.0650\n",
      "    Round 2: 0.0850\n",
      "    Round 3: 0.1100\n",
      "    Round 4: 0.1150\n",
      "    Round 5: 0.0900\n",
      "    Round 6: 0.1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Round 7: 0.1650\n",
      "    Round 8: 0.0750\n",
      "    Round 9: 0.0950\n",
      "  Average: 0.1095\n",
      "Testing aux_weight 2.0, batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.1150\n",
      "    Round 1: 0.0900\n",
      "    Round 2: 0.0950\n",
      "    Round 3: 0.1150\n",
      "    Round 4: 0.1250\n",
      "    Round 5: 0.1500\n",
      "    Round 6: 0.0800\n",
      "    Round 7: 0.1450\n",
      "    Round 8: 0.1350\n",
      "    Round 9: 0.1200\n",
      "  Average: 0.1170\n",
      "Testing aux_weight 2.0, batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.0750\n",
      "    Round 1: 0.1200\n",
      "    Round 2: 0.1800\n",
      "    Round 3: 0.1200\n",
      "    Round 4: 0.1450\n",
      "    Round 5: 0.1050\n",
      "    Round 6: 0.1100\n",
      "    Round 7: 0.1300\n",
      "    Round 8: 0.1550\n",
      "    Round 9: 0.1350\n",
      "  Average: 0.1275\n",
      "Testing aux_weight 5.0, batch_size 25, lr=0.001, units=10\n",
      "    Round 0: 0.1150\n",
      "    Round 1: 0.0950\n",
      "    Round 2: 0.1250\n",
      "    Round 3: 0.1100\n",
      "    Round 4: 0.1150\n",
      "    Round 5: 0.0900\n",
      "    Round 6: 0.1200\n",
      "    Round 7: 0.0950\n",
      "    Round 8: 0.1350\n",
      "    Round 9: 0.1200\n",
      "  Average: 0.1120\n",
      "Testing aux_weight 5.0, batch_size 25, lr=0.001, units=25\n",
      "    Round 0: 0.1450\n",
      "    Round 1: 0.1200\n",
      "    Round 2: 0.1100\n",
      "    Round 3: 0.1100\n",
      "    Round 4: 0.1400\n",
      "    Round 5: 0.0900\n",
      "    Round 6: 0.0700\n",
      "    Round 7: 0.1000\n",
      "    Round 8: 0.1400\n",
      "    Round 9: 0.1150\n",
      "  Average: 0.1140\n",
      "Testing aux_weight 5.0, batch_size 25, lr=0.001, units=50\n",
      "    Round 0: 0.0400\n",
      "    Round 1: 0.0850\n",
      "    Round 2: 0.1100\n",
      "    Round 3: 0.0700\n",
      "    Round 4: 0.1050\n",
      "    Round 5: 0.0950\n",
      "    Round 6: 0.1450\n",
      "    Round 7: 0.1250\n",
      "    Round 8: 0.0650\n",
      "    Round 9: 0.1050\n",
      "  Average: 0.0945\n",
      "Testing aux_weight 5.0, batch_size 25, lr=0.001, units=100\n",
      "    Round 0: 0.1350\n",
      "    Round 1: 0.0900\n",
      "    Round 2: 0.0800\n",
      "    Round 3: 0.1250\n",
      "    Round 4: 0.0700\n",
      "    Round 5: 0.1100\n",
      "    Round 6: 0.1150\n",
      "    Round 7: 0.1000\n",
      "    Round 8: 0.0700\n",
      "    Round 9: 0.0750\n",
      "  Average: 0.0970\n",
      "Testing aux_weight 5.0, batch_size 25, lr=0.01, units=10\n",
      "    Round 0: 0.0950\n",
      "    Round 1: 0.0950\n",
      "    Round 2: 0.1700\n",
      "    Round 3: 0.0800\n",
      "    Round 4: 0.1550\n",
      "    Round 5: 0.1350\n",
      "    Round 6: 0.1150\n",
      "    Round 7: 0.1250\n",
      "    Round 8: 0.1200\n",
      "    Round 9: 0.0900\n",
      "  Average: 0.1180\n",
      "Testing aux_weight 5.0, batch_size 25, lr=0.01, units=25\n",
      "    Round 0: 0.1400\n",
      "    Round 1: 0.1300\n",
      "    Round 2: 0.0650\n",
      "    Round 3: 0.1450\n",
      "    Round 4: 0.0950\n",
      "    Round 5: 0.1100\n",
      "    Round 6: 0.0950\n",
      "    Round 7: 0.1150\n",
      "    Round 8: 0.1600\n",
      "    Round 9: 0.0950\n",
      "  Average: 0.1150\n",
      "Testing aux_weight 5.0, batch_size 25, lr=0.01, units=50\n",
      "    Round 0: 0.0950\n",
      "    Round 1: 0.1250\n",
      "    Round 2: 0.1000\n",
      "    Round 3: 0.1950\n",
      "    Round 4: 0.1850\n",
      "    Round 5: 0.1550\n",
      "    Round 6: 0.1100\n",
      "    Round 7: 0.1350\n",
      "    Round 8: 0.1350\n",
      "    Round 9: 0.4150\n",
      "  Average: 0.1650\n",
      "Testing aux_weight 5.0, batch_size 25, lr=0.01, units=100\n",
      "    Round 0: 0.1300\n",
      "    Round 1: 0.1000\n",
      "    Round 2: 0.1150\n",
      "    Round 3: 0.1400\n",
      "    Round 4: 0.0800\n",
      "    Round 5: 0.1100\n",
      "    Round 6: 0.1100\n",
      "    Round 7: 0.1350\n",
      "    Round 8: 0.1350\n",
      "    Round 9: 0.1300\n",
      "  Average: 0.1185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 0.001, 50, 5.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "rounds = 10\n",
    "seed = 0\n",
    "\n",
    "aux_loss_weights = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "learning_rates = [0.001, 0.01]\n",
    "batch_sizes = [25]\n",
    "hidden_layer_units = [10, 25, 50, 100]\n",
    "\n",
    "aux_loss_weight, batch_size, lr, hidden_units = model_tuning_aux_loss(\n",
    "    gen_weight_sharing_aux_loss, train_with_auxiliary_loss, epochs, rounds,\n",
    "    batch_sizes, learning_rates, hidden_layer_units, aux_loss_weights, seed=seed)\n",
    "\n",
    "aux_loss_weight, batch_size, lr, hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-check",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
